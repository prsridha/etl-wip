{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82da616e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cerebro'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcerebro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdask_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DaskBackend\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcerebro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_info\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcerebro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparams\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Params\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cerebro'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from PIL import Image\n",
    "from dask.distributed import Client\n",
    "from cerebro.dask_backend import DaskBackend\n",
    "from cerebro.dataset_info import DatasetInfo\n",
    "from cerebro.params import Params\n",
    "from cerebro.etl import etl\n",
    "import cerebro.constants as constants\n",
    "from torchvision import transforms\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import matplotlib\n",
    "from distributed.diagnostics import MemorySampler\n",
    "from dask.distributed import performance_report\n",
    "\n",
    "def prepare_data():\n",
    "    data = None\n",
    "    with open(\"/mydata/coco/annotations/captions_val2014.json\") as f:\n",
    "        data = json.load(f)\n",
    "    dataset = {\n",
    "        'id': [],\n",
    "        'file_name': [],\n",
    "        'height': [],\n",
    "        'width': [],\n",
    "        'captions': [],\n",
    "        'date_captured': [] \n",
    "    }\n",
    "\n",
    "    annotations = {}\n",
    "    annotations_list = data['annotations']\n",
    "    for i in annotations_list:\n",
    "        if not i[\"image_id\"] in annotations:\n",
    "            annotations[i[\"image_id\"]] = []\n",
    "        annotations[i[\"image_id\"]].append(i[\"caption\"])\n",
    "\n",
    "    for i in range(len(data['images'])):\n",
    "        dataset['id'].append(data[\"images\"][i]['id'])\n",
    "        dataset['file_name'].append(data[\"images\"][i]['file_name'])\n",
    "        dataset['height'].append(data[\"images\"][i]['height'])\n",
    "        dataset['width'].append(data[\"images\"][i]['width'])\n",
    "        dataset['captions'].append(annotations[data[\"images\"][i]['id']])\n",
    "        dataset['date_captured'].append(data[\"images\"][i]['date_captured'])\n",
    "\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    dataset.to_csv(\"/mydata/coco/annotations/captions_val2014_modified.csv\", index=False)\n",
    "\n",
    "def row_preprocessing_routine(row, to_root_path, kwargs):\n",
    "    t1 = time.time()\n",
    "    input_image_path = to_root_path + str(row[\"file_name\"])\n",
    "    output_caption = row[\"captions\"]\n",
    "    img = Image.open(input_image_path)\n",
    "    img_tensor = transforms.PILToTensor()(img)\n",
    "    enc_model = kwargs['nlp_model']\n",
    "    caption_tensor = enc_model.encode([output_caption], convert_to_tensor=True)\n",
    "    saved = [img_tensor, caption_tensor]\n",
    "    t2 = time.time()\n",
    "    return [ kwargs[\"io_time\"], t2-t1]\n",
    "    \n",
    "def testing():\n",
    "    prepare_data()\n",
    "    df = pd.read_csv(\"/mydata/coco/annotations/captions_val2014_modified.csv\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3d2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd9c4028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client dashboard:  http://0.0.0.0:8787/status\n",
      "Number of workers: 3\n",
      "40504\n"
     ]
    }
   ],
   "source": [
    "dsk_bknd = DaskBackend(\"0.0.0.0:8786\")\n",
    "\n",
    "prepare_data()\n",
    "is_feature_download = [False, True, False, False, False, False]\n",
    "feature_names = [\"id\", \"file_name\", \"height\", \"width\", \"captions\", \"date_captured\"]\n",
    "dtypes = (int, str, int, int, list, str)\n",
    "data_info = DatasetInfo(feature_names, feature_names, [], dtypes, is_feature_download)\n",
    "\n",
    "metadata_path = \"/mydata/coco/annotations/captions_val2014_modified.csv\"\n",
    "from_root_path = \"/mydata/coco/images/val2014/\"\n",
    "to_root_path = \"/mydata/coco/val2014/\"\n",
    "output_path = \"\"\n",
    "requirements_path = \"\"\n",
    "download_type = constants.DOWNLOAD_FROM_SERVER\n",
    "username = \"vik1497\"\n",
    "host = \"128.110.218.13\"\n",
    "pem_path = \"/users/vik1497/cloudlab.pem\"\n",
    "\n",
    "nlp_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "params = Params(metadata_path, from_root_path, to_root_path,\n",
    "    output_path, requirements_path, username, host, pem_path,\n",
    "    download_type)\n",
    "\n",
    "e = etl(dsk_bknd, params, row_preprocessing_routine, data_info)\n",
    "\n",
    "e.load_data(frac=1)\n",
    "e.shuffle_shard_data()\n",
    "e.sharded_df.compute()\n",
    "print(len(e.sharded_df))\n",
    "result = e.preprocess_data(nlp_model=nlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6fcf22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with performance_report(filename=\"report_3_workers.html\"):\n",
    "    out = result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a011f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = out_df.copy()\n",
    "out1[[\"io\", \"cpu\"]] = list(out_df[\"transformed_data\"])\n",
    "out1 = out1.drop(\"transformed_data\", axis=1)\n",
    "out1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1[\"io\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97162f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1[\"cpu\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4866c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43480aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MemorySampler()\n",
    "with ms.sample(\"Result\"):\n",
    "    out = result.compute()\n",
    "    \n",
    "out_df = out.to_frame()\n",
    "ms.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe82a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494afd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
