{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "88ff0bb1-0388-40a3-ae7e-55aebbb705b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from vocabulary import Vocabulary\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "\n",
    "# python3 -m pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "01b954a1-5aca-47ac-8d44-7d9c90b2322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to pre-process the training images.\n",
    "\n",
    "\n",
    "# Set the minimum word count threshold.\n",
    "vocab_threshold = 5\n",
    "\n",
    "# Specify the batch size.\n",
    "batch_size = 5\n",
    "\n",
    "base_data_path = '/Users/vignesh/Desktop/code/cerebro_etl_stuff/coco'\n",
    "train_annotations_path = base_data_path + \"/annotations/captions_train2014.json\"\n",
    "val_annotations_path = base_data_path + \"/annotations/captions_val2014.json\"\n",
    "images_path = base_data_path + \"/train2014/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3ba89462-b07e-4ef1-a132-43231c5aa708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n"
     ]
    }
   ],
   "source": [
    "# loading vocabulary for annotations\n",
    "vocab = Vocabulary(vocab_threshold, vocab_from_file=True) # loading from file (change to annotations_file attribute?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5efa8f89-d29a-417e-bb03-cbe6f0830db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row preprocessing function: processing images to tensors and captions to their index vectors (using vocab)\n",
    "def row_preprocessing_routine(row, to_root_path, **kwargs):\n",
    "    from torchvision import transforms\n",
    "    \n",
    "    # Convert image to tensor and pre-process using transform\n",
    "    transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # print(row)\n",
    "    # print(str(row[\"file_name\"]))\n",
    "    # print(row[\"captions\"])\n",
    "    input_image_path = to_root_path + str(row[\"file_name\"])\n",
    "    image = Image.open(input_image_path).convert(\"RGB\")\n",
    "    image_tensor = transform_train(image)\n",
    "    \n",
    "    vocab = kwargs[\"vocab\"]\n",
    "    \n",
    "    output_caption = row[\"captions\"]\n",
    "    tokens = nltk.tokenize.word_tokenize(str(output_caption).lower())\n",
    "    caption = []\n",
    "    caption.append(vocab(vocab.start_word))\n",
    "    caption.extend([vocab(token) for token in tokens])\n",
    "    caption.append(vocab(vocab.end_word))\n",
    "    caption_tensor = torch.Tensor(caption).long()\n",
    "    return image_tensor, caption_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a87029c6-f43a-4398-a974-ec8a396cfc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the metadata data frame\n",
    "data_json = None\n",
    "with open(train_annotations_path) as f:\n",
    "    data_json = json.load(f)\n",
    "feature_names = [\"id\", \"file_name\", \"height\", \"width\", \"caption\", \"date_captured\"]\n",
    "dataset = {\n",
    "    'id': [],\n",
    "    'file_name': [],\n",
    "    'height': [],\n",
    "    'width': [],\n",
    "    'captions': [],\n",
    "    'date_captured': []\n",
    "}\n",
    "\n",
    "dataset_modified = {\n",
    "    'id': [],\n",
    "    'file_name': [],\n",
    "    'height': [],\n",
    "    'width': [],\n",
    "    'captions': [],\n",
    "    'date_captured': []\n",
    "}\n",
    "\n",
    "annotations = {}\n",
    "annotations_list = data_json['annotations']\n",
    "for i in annotations_list:\n",
    "    if not i[\"image_id\"] in annotations:\n",
    "        annotations[i[\"image_id\"]] = []\n",
    "    annotations[i[\"image_id\"]].append(i[\"caption\"])\n",
    "\n",
    "for i in range(len(data_json['images'])):\n",
    "    for caption in annotations[data_json[\"images\"][i]['id']]:\n",
    "        dataset['id'].append(data_json[\"images\"][i]['id'])\n",
    "        dataset['file_name'].append(data_json[\"images\"][i]['file_name'])\n",
    "        dataset['height'].append(data_json[\"images\"][i]['height'])\n",
    "        dataset['width'].append(data_json[\"images\"][i]['width'])\n",
    "        dataset['captions'].append(caption)\n",
    "        dataset['date_captured'].append(data_json[\"images\"][i]['date_captured'])\n",
    "\n",
    "pd_df = pd.DataFrame(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f1ca98fa-f5e1-4a3c-b809-a840d947bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "# image_tensor, caption_tensor = row_preprocessing_routine(pd_df.iloc[[0]], images_path, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2efd74e8-9b02-4d9a-8748-7e00cd569588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>captions</th>\n",
       "      <th>date_captured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57870</td>\n",
       "      <td>COCO_train2014_000000057870.jpg</td>\n",
       "      <td>480</td>\n",
       "      <td>640</td>\n",
       "      <td>A restaurant has modern wooden tables and chairs.</td>\n",
       "      <td>2013-11-14 16:28:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57870</td>\n",
       "      <td>COCO_train2014_000000057870.jpg</td>\n",
       "      <td>480</td>\n",
       "      <td>640</td>\n",
       "      <td>A long restaurant table with rattan rounded ba...</td>\n",
       "      <td>2013-11-14 16:28:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57870</td>\n",
       "      <td>COCO_train2014_000000057870.jpg</td>\n",
       "      <td>480</td>\n",
       "      <td>640</td>\n",
       "      <td>a long table with a plant on top of it surroun...</td>\n",
       "      <td>2013-11-14 16:28:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57870</td>\n",
       "      <td>COCO_train2014_000000057870.jpg</td>\n",
       "      <td>480</td>\n",
       "      <td>640</td>\n",
       "      <td>A long table with a flower arrangement in the ...</td>\n",
       "      <td>2013-11-14 16:28:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57870</td>\n",
       "      <td>COCO_train2014_000000057870.jpg</td>\n",
       "      <td>480</td>\n",
       "      <td>640</td>\n",
       "      <td>A table is adorned with wooden chairs with blu...</td>\n",
       "      <td>2013-11-14 16:28:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>384029</td>\n",
       "      <td>COCO_train2014_000000384029.jpg</td>\n",
       "      <td>429</td>\n",
       "      <td>640</td>\n",
       "      <td>A man preparing desserts in a kitchen covered ...</td>\n",
       "      <td>2013-11-14 16:29:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>384029</td>\n",
       "      <td>COCO_train2014_000000384029.jpg</td>\n",
       "      <td>429</td>\n",
       "      <td>640</td>\n",
       "      <td>A chef is preparing and decorating many small ...</td>\n",
       "      <td>2013-11-14 16:29:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>384029</td>\n",
       "      <td>COCO_train2014_000000384029.jpg</td>\n",
       "      <td>429</td>\n",
       "      <td>640</td>\n",
       "      <td>A baker prepares various types of baked goods.</td>\n",
       "      <td>2013-11-14 16:29:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>384029</td>\n",
       "      <td>COCO_train2014_000000384029.jpg</td>\n",
       "      <td>429</td>\n",
       "      <td>640</td>\n",
       "      <td>a close up of a person grabbing a pastry in a ...</td>\n",
       "      <td>2013-11-14 16:29:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>384029</td>\n",
       "      <td>COCO_train2014_000000384029.jpg</td>\n",
       "      <td>429</td>\n",
       "      <td>640</td>\n",
       "      <td>Close up of a hand touching various pastries.</td>\n",
       "      <td>2013-11-14 16:29:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>222016</td>\n",
       "      <td>COCO_train2014_000000222016.jpg</td>\n",
       "      <td>640</td>\n",
       "      <td>480</td>\n",
       "      <td>a big red telephone booth that a man is standi...</td>\n",
       "      <td>2013-11-14 16:37:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>222016</td>\n",
       "      <td>COCO_train2014_000000222016.jpg</td>\n",
       "      <td>640</td>\n",
       "      <td>480</td>\n",
       "      <td>a person standing inside of a phone booth</td>\n",
       "      <td>2013-11-14 16:37:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>222016</td>\n",
       "      <td>COCO_train2014_000000222016.jpg</td>\n",
       "      <td>640</td>\n",
       "      <td>480</td>\n",
       "      <td>this is an image of a man in a phone booth.</td>\n",
       "      <td>2013-11-14 16:37:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>222016</td>\n",
       "      <td>COCO_train2014_000000222016.jpg</td>\n",
       "      <td>640</td>\n",
       "      <td>480</td>\n",
       "      <td>A man is standing in a red phone booth.</td>\n",
       "      <td>2013-11-14 16:37:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>222016</td>\n",
       "      <td>COCO_train2014_000000222016.jpg</td>\n",
       "      <td>640</td>\n",
       "      <td>480</td>\n",
       "      <td>A man using a phone in a phone booth.</td>\n",
       "      <td>2013-11-14 16:37:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>520950</td>\n",
       "      <td>COCO_train2014_000000520950.jpg</td>\n",
       "      <td>427</td>\n",
       "      <td>640</td>\n",
       "      <td>the kitchen is full of spices on the rack</td>\n",
       "      <td>2013-11-14 16:44:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>520950</td>\n",
       "      <td>COCO_train2014_000000520950.jpg</td>\n",
       "      <td>427</td>\n",
       "      <td>640</td>\n",
       "      <td>A kitchen with counter, oven and other accesso...</td>\n",
       "      <td>2013-11-14 16:44:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>520950</td>\n",
       "      <td>COCO_train2014_000000520950.jpg</td>\n",
       "      <td>427</td>\n",
       "      <td>640</td>\n",
       "      <td>A small kitchen that utilizes all of its space.</td>\n",
       "      <td>2013-11-14 16:44:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>520950</td>\n",
       "      <td>COCO_train2014_000000520950.jpg</td>\n",
       "      <td>427</td>\n",
       "      <td>640</td>\n",
       "      <td>This small kitchen has pots, pans and spices o...</td>\n",
       "      <td>2013-11-14 16:44:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>520950</td>\n",
       "      <td>COCO_train2014_000000520950.jpg</td>\n",
       "      <td>427</td>\n",
       "      <td>640</td>\n",
       "      <td>A VERY SMALL KITCHEN WITH A STOVE AND A SHELF ...</td>\n",
       "      <td>2013-11-14 16:44:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                        file_name  height  width  \\\n",
       "0    57870  COCO_train2014_000000057870.jpg     480    640   \n",
       "1    57870  COCO_train2014_000000057870.jpg     480    640   \n",
       "2    57870  COCO_train2014_000000057870.jpg     480    640   \n",
       "3    57870  COCO_train2014_000000057870.jpg     480    640   \n",
       "4    57870  COCO_train2014_000000057870.jpg     480    640   \n",
       "5   384029  COCO_train2014_000000384029.jpg     429    640   \n",
       "6   384029  COCO_train2014_000000384029.jpg     429    640   \n",
       "7   384029  COCO_train2014_000000384029.jpg     429    640   \n",
       "8   384029  COCO_train2014_000000384029.jpg     429    640   \n",
       "9   384029  COCO_train2014_000000384029.jpg     429    640   \n",
       "10  222016  COCO_train2014_000000222016.jpg     640    480   \n",
       "11  222016  COCO_train2014_000000222016.jpg     640    480   \n",
       "12  222016  COCO_train2014_000000222016.jpg     640    480   \n",
       "13  222016  COCO_train2014_000000222016.jpg     640    480   \n",
       "14  222016  COCO_train2014_000000222016.jpg     640    480   \n",
       "15  520950  COCO_train2014_000000520950.jpg     427    640   \n",
       "16  520950  COCO_train2014_000000520950.jpg     427    640   \n",
       "17  520950  COCO_train2014_000000520950.jpg     427    640   \n",
       "18  520950  COCO_train2014_000000520950.jpg     427    640   \n",
       "19  520950  COCO_train2014_000000520950.jpg     427    640   \n",
       "\n",
       "                                             captions        date_captured  \n",
       "0   A restaurant has modern wooden tables and chairs.  2013-11-14 16:28:13  \n",
       "1   A long restaurant table with rattan rounded ba...  2013-11-14 16:28:13  \n",
       "2   a long table with a plant on top of it surroun...  2013-11-14 16:28:13  \n",
       "3   A long table with a flower arrangement in the ...  2013-11-14 16:28:13  \n",
       "4   A table is adorned with wooden chairs with blu...  2013-11-14 16:28:13  \n",
       "5   A man preparing desserts in a kitchen covered ...  2013-11-14 16:29:45  \n",
       "6   A chef is preparing and decorating many small ...  2013-11-14 16:29:45  \n",
       "7      A baker prepares various types of baked goods.  2013-11-14 16:29:45  \n",
       "8   a close up of a person grabbing a pastry in a ...  2013-11-14 16:29:45  \n",
       "9       Close up of a hand touching various pastries.  2013-11-14 16:29:45  \n",
       "10  a big red telephone booth that a man is standi...  2013-11-14 16:37:59  \n",
       "11         a person standing inside of a phone booth   2013-11-14 16:37:59  \n",
       "12        this is an image of a man in a phone booth.  2013-11-14 16:37:59  \n",
       "13            A man is standing in a red phone booth.  2013-11-14 16:37:59  \n",
       "14              A man using a phone in a phone booth.  2013-11-14 16:37:59  \n",
       "15          the kitchen is full of spices on the rack  2013-11-14 16:44:40  \n",
       "16  A kitchen with counter, oven and other accesso...  2013-11-14 16:44:40  \n",
       "17   A small kitchen that utilizes all of its space.   2013-11-14 16:44:40  \n",
       "18  This small kitchen has pots, pans and spices o...  2013-11-14 16:44:40  \n",
       "19  A VERY SMALL KITCHEN WITH A STOVE AND A SHELF ...  2013-11-14 16:44:40  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_df = pd_df.head(20)\n",
    "small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fbb3df58-322c-4690-ae28-3c9b02aff19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "ret = small_df.apply(lambda row: row_preprocessing_routine(row, images_path, vocab=vocab), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc12323b-65ce-4912-9557-c2cfbda82343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ret.iloc[[0]][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0964f519-3d41-4d10-9280-232b795d06ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[tensor(-1.3987), tensor(-1.4158), tensor(-1...</td>\n",
       "      <td>[tensor(0), tensor(3), tensor(535), tensor(105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[tensor(-0.7308), tensor(-0.7479), tensor(-0...</td>\n",
       "      <td>[tensor(0), tensor(3), tensor(890), tensor(535...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[tensor(-0.1486), tensor(-0.6794), tensor(-1...</td>\n",
       "      <td>[tensor(0), tensor(3), tensor(890), tensor(112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[tensor(0.5022), tensor(0.4679), tensor(0.45...</td>\n",
       "      <td>[tensor(0), tensor(3), tensor(890), tensor(112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[tensor(2.2147), tensor(2.2147), tensor(2.19...</td>\n",
       "      <td>[tensor(0), tensor(3), tensor(112), tensor(130...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [[[tensor(-1.3987), tensor(-1.4158), tensor(-1...   \n",
       "1  [[[tensor(-0.7308), tensor(-0.7479), tensor(-0...   \n",
       "2  [[[tensor(-0.1486), tensor(-0.6794), tensor(-1...   \n",
       "3  [[[tensor(0.5022), tensor(0.4679), tensor(0.45...   \n",
       "4  [[[tensor(2.2147), tensor(2.2147), tensor(2.19...   \n",
       "\n",
       "                                                   1  \n",
       "0  [tensor(0), tensor(3), tensor(535), tensor(105...  \n",
       "1  [tensor(0), tensor(3), tensor(890), tensor(535...  \n",
       "2  [tensor(0), tensor(3), tensor(890), tensor(112...  \n",
       "3  [tensor(0), tensor(3), tensor(890), tensor(112...  \n",
       "4  [tensor(0), tensor(3), tensor(112), tensor(130...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "13ed6331-c5fe-47ef-a218-0a890eb2b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html\n",
    "pkl_data_file = \"./small_tensor_data.pkl\"\n",
    "ret.to_pickle(pkl_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5606d00-2ba0-4079-a9c9-d1e9ca83eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df = pd.read_pickle(pkl_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c35cb012-75ff-4f63-af7b-12cd7595b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralPytorchDataset(data.Dataset):\n",
    "    def __init__(self, pickled_data_file, batch_size):\n",
    "        self.data_df = pd.read_pickle(pickled_data_file)\n",
    "        self.caption_lengths = [caption.size(dim=0) for caption in self.data_df[[1]][1]] \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = self.data_df[[0]][0][index]\n",
    "        caption = self.data_df[[1]][1][index]\n",
    "        # sample = {'image': image, 'caption': caption}\n",
    "        return image, caption\n",
    "    \n",
    "    def get_indices(self):\n",
    "        selected_length = np.random.choice(self.caption_lengths)\n",
    "        all_indices = np.where([self.caption_lengths[i] == \\\n",
    "                               selected_length for i in np.arange(len(self.caption_lengths))])[0]\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c95c5afa-7902-4db5-b491-3e1aaec1d78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([3, 224, 224]) 11\n",
      "1 torch.Size([3, 224, 224]) 12\n",
      "2 torch.Size([3, 224, 224]) 16\n",
      "3 torch.Size([3, 224, 224]) 14\n",
      "4 torch.Size([3, 224, 224]) 13\n",
      "5 torch.Size([3, 224, 224]) 13\n",
      "6 torch.Size([3, 224, 224]) 12\n",
      "7 torch.Size([3, 224, 224]) 11\n",
      "8 torch.Size([3, 224, 224]) 14\n",
      "9 torch.Size([3, 224, 224]) 11\n",
      "10 torch.Size([3, 224, 224]) 13\n",
      "11 torch.Size([3, 224, 224]) 10\n",
      "12 torch.Size([3, 224, 224]) 14\n",
      "13 torch.Size([3, 224, 224]) 12\n",
      "14 torch.Size([3, 224, 224]) 12\n",
      "15 torch.Size([3, 224, 224]) 11\n",
      "16 torch.Size([3, 224, 224]) 12\n",
      "17 torch.Size([3, 224, 224]) 12\n",
      "18 torch.Size([3, 224, 224]) 13\n",
      "19 torch.Size([3, 224, 224]) 14\n"
     ]
    }
   ],
   "source": [
    "coco_dataset = GeneralPytorchDataset(pkl_data_file, batch_size)\n",
    "for i in range(len(coco_dataset)):\n",
    "    image, caption = coco_dataset[i]\n",
    "    print(i, image.size(), caption.size(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b6a9a94d-08a3-4940-adf4-9093b2da2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = coco_dataset.get_indices()\n",
    "initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "data_loader = data.DataLoader(dataset=coco_dataset, \n",
    "                              num_workers=0,\n",
    "                              batch_sampler=data.sampler.BatchSampler(sampler=initial_sampler,\n",
    "                                                                      batch_size=coco_dataset.batch_size,\n",
    "                                                                      drop_last=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7717bbc3-99e1-46f4-ac6c-24e8679d5c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 12 --- count:     6\n",
      "value: 11 --- count:     4\n",
      "value: 14 --- count:     4\n",
      "value: 13 --- count:     4\n",
      "value: 16 --- count:     1\n",
      "value: 10 --- count:     1\n"
     ]
    }
   ],
   "source": [
    "# dataloader = DataLoader(coco_dataset, batch_size=batch_size,shuffle=True, num_workers=0)\n",
    "from collections import Counter\n",
    "counter = Counter(data_loader.dataset.caption_lengths)\n",
    "lengths = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
    "for value, count in lengths:\n",
    "    print('value: %2d --- count: %5d' % (value, count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fd7ee4f2-960d-48d2-b176-bf3f08bfa6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([5, 3, 224, 224])\n",
      "captions.shape: torch.Size([5, 13])\n"
     ]
    }
   ],
   "source": [
    "indices = data_loader.dataset.get_indices()\n",
    "new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "data_loader.batch_sampler.sampler = new_sampler\n",
    "for batch in data_loader:\n",
    "    images, captions = batch[0], batch[1]\n",
    "    print('images.shape:', images.shape)\n",
    "    print('captions.shape:', captions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4006890a-c6b8-4db4-a2d9-1343162afaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_pickle(pkl_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b0be6cec-7268-4455-b545-be6be9ac3b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11,\n",
       " 12,\n",
       " 16,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 11,\n",
       " 14,\n",
       " 11,\n",
       " 13,\n",
       " 10,\n",
       " 14,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 14]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[caption.size(dim=0) for caption in data_df[[1]][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "651e18d1-3f4e-4dc7-90a9-1ffe4502f4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/vignesh/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████████████| 97.8M/97.8M [00:03<00:00, 29.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Specify the dimensionality of the image embedding.\n",
    "embed_size = 256\n",
    "\n",
    "# Initialize the encoder. (We can add additional arguments if necessary.)\n",
    "encoder = EncoderCNN(embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "952293c8-e619-4de0-991c-aecdb568b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = encoder(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c38a641f-eba2-4d42-a684-41a667776d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(features): <class 'torch.Tensor'>\n",
      "features.shape: torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "print('type(features):', type(features))\n",
    "print('features.shape:', features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2120c2f5-9d1e-4ca3-bded-a6149ab76a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (features.shape[0]==batch_size) & (features.shape[1]==embed_size), \"The shape of the encoder output is incorrect.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1bfdef1a-e17a-4c80-98e1-d96b538557ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3fddf500-1180-4e05-93b0-1ab0f969219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c1d05474-b60c-407a-a3d0-dad732b467d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = decoder(features, captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f276d65b-5551-42ba-ac42-9dcab6fc1ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(outputs): <class 'torch.Tensor'>\n",
      "outputs.shape: torch.Size([5, 13, 8855])\n"
     ]
    }
   ],
   "source": [
    "print('type(outputs):', type(outputs))\n",
    "print('outputs.shape:', outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c5283407-d2a6-4e2e-996a-1722c82d75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (outputs.shape[0]==batch_size) & (outputs.shape[1]==captions.shape[1]) & (outputs.shape[2]==vocab_size), \"The shape of the decoder output is incorrect.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d353d7-ba65-4621-a313-ad60891036fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
